# Pulse Fetch MCP Server Configuration
# Copy this file to .env and fill in your actual values

# Firecrawl API Key (optional)
# Get one at: https://www.firecrawl.dev/
FIRECRAWL_API_KEY=your-firecrawl-api-key-here

# Firecrawl API Base URL (optional)
# Custom base URL for Firecrawl API (useful for self-hosted instances)
# Defaults to: https://api.firecrawl.dev
# FIRECRAWL_API_BASE_URL=https://api.firecrawl.dev

# BrightData API Key (optional)  
# Get one at: https://brightdata.com/ from the Web Unlocker product
# Just provide the token - 'Bearer ' will be prepended automatically
BRIGHTDATA_API_KEY=your-brightdata-token-here

# Strategy Configuration File Path (optional)
# Path to the markdown file containing scraping strategy configuration
# If not set, will use a default file in your OS temp directory
# PULSE_FETCH_STRATEGY_CONFIG_PATH=/path/to/your/scraping-strategies.md

# Optimization Strategy (optional)
# Controls the order and selection of scraping strategies
# Valid values: cost (default), speed
# - cost: Tries native first, then firecrawl, then brightdata (optimizes for lowest cost)
# - speed: Tries firecrawl first, then brightdata (skips native for faster results)
# OPTIMIZE_FOR=cost

# Resource Storage Configuration (optional)
# Controls how scraped resources are stored
# Valid values: memory (default), filesystem
# - memory: Stores resources in memory (lost on restart)
# - filesystem: Persists resources to disk
# MCP_RESOURCE_STORAGE=memory

# Filesystem Resource Storage Root (optional)
# Only used when MCP_RESOURCE_STORAGE=filesystem
# Directory where resources will be stored
# Defaults to: /tmp/pulse-fetch/resources (or OS equivalent)
# MCP_RESOURCE_FILESYSTEM_ROOT=/path/to/resource/storage

# LLM Provider Configuration (optional - for extract feature)
# Controls which LLM provider to use for intelligent content extraction
# This provides an alternative to MCP sampling for clients that don't support it

# LLM Provider Type (optional)
# Valid values: anthropic, openai, openai-compatible
# - anthropic: Use Anthropic's Claude models directly
# - openai: Use OpenAI's GPT models
# - openai-compatible: Use any OpenAI-compatible provider (Together.ai, Groq, etc.)
# LLM_PROVIDER=anthropic

# LLM API Key (optional)
# The API key for your chosen LLM provider
# - For Anthropic: Get from https://console.anthropic.com/
# - For OpenAI: Get from https://platform.openai.com/
# - For others: Check your provider's documentation
# LLM_API_KEY=your-llm-api-key-here

# LLM API Base URL (optional)
# Only needed for openai-compatible providers
# Examples:
# - Together.ai: https://api.together.xyz/v1
# - Groq: https://api.groq.com/openai/v1
# - Perplexity: https://api.perplexity.ai
# LLM_API_BASE_URL=https://api.together.xyz/v1

# LLM Model (optional)
# The specific model to use for extraction
# Defaults:
# - Anthropic: claude-sonnet-4-20250514 (latest and most capable)
# - OpenAI: gpt-4.1-mini (latest and most capable)
# - OpenAI-compatible: Must be specified
# Examples:
# - Anthropic: claude-sonnet-4-20250514, claude-opus-4-20250514, claude-3-7-sonnet-20250219, claude-3-5-haiku-20241022
# - OpenAI: gpt-4.1-mini, gpt-4.1, gpt-4.1-nano, gpt-4o, gpt-4o-mini
# - Together.ai: meta-llama/Llama-3-70b-chat-hf
# LLM_MODEL=claude-sonnet-4-20250514
